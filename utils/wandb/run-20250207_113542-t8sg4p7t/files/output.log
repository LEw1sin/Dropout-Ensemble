2025-02-07 11:35:43,342 - INFO - Namespace(device='cuda:1', channel_weights=[1.0, 2.0, 1.0, 1.0], batch_size=2, num_classes=4, pretrained=False, pretrained_weights='./Synapse_weights_1u1d', lr=0.0001, epochs=500, warmup_epochs=10, dataset_mode='ACDC', max_channel=256, l2_norm=1e-07, patience=15, wandb=True, cache=True, ed_es_only='', log_dir='../../de_logistics/ACDC_1UNetlinear_02-07-11-35-42/', log_file_path='../../de_logistics/ACDC_1UNetlinear_02-07-11-35-42/training.log')
Epoch 1/500 [Training]:  31%|████████████████▉                                     | 47/150 [00:08<00:19,  5.38batch/s]
Traceback (most recent call last):
  File "/data2/lyw/deunet_mem_based/train.py", line 229, in <module>
    main(args, net)
  File "/data2/lyw/deunet_mem_based/train.py", line 70, in main
    train_acc, train_losses = train_epoch(args, net, optimizer, train_loader, device, epoch, scheduler)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data2/lyw/deunet_mem_based/train.py", line 141, in train_epoch
    train_losses, train_acc, _= process_epoch(args, net, train_loader, device, 'train', optimizer, scaler, scheduler)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data2/lyw/deunet_mem_based/train.py", line 116, in process_epoch
    scaler.scale(loss).backward()
  File "/home/lyw/anaconda3/envs/unet/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/lyw/anaconda3/envs/unet/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/lyw/anaconda3/envs/unet/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
